{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956aa214",
   "metadata": {},
   "source": [
    "# Ensemble\n",
    "`06_ensemble.ipynb`\n",
    "\n",
    "- 정형 데이터 (Structured Data) 기준으로는 가장 뛰어난 상과를 내는 알고리즘 -> 앙상블 학습\n",
    "- 대부분의 앙상블 학습 -> 트리 기반\n",
    "\n",
    "## Random Forest\n",
    "- 결정트리를 랜덤하게 만들어 트리의 숲을 만듦\n",
    "- 각 결정트리의 예측을 종합해 최종 예측을 만듦\n",
    "- 과대적합(overfitting)에 안전!\n",
    "\n",
    "### 데이터 분할\n",
    "- 데이터가 1000개면, 각 트리마다 1000개 데이터를 복원추출함. 이때 중복을 허용 (우연히 같은데이터만 1000개 가능)\n",
    "- 노드 분할시, 분류 / 회귀의 특성 선택방식이 다름(분류: 개수를 루트함, 회귀: 특성을 다 씀)\n",
    "- 기본값 100개의 트리를 만들어서\n",
    "    - 분류: 다수결 투표\n",
    "    - 회귀: 100개의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wine = pd.read_csv('./wine.csv')\n",
    "X = wine[['alcohol', 'sugar', 'pH']]\n",
    "y = wine['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c414e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# 교차검증(5 fold)\n",
    "scores = cross_validate(rf, X_train, y_train, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "# 결정트리가 max_depth 튜닝이 없으면 Overfitting 됨 \n",
    "#       훈련셋(일부)                        검증셋(훈련셋의 일부)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "# 진짜 학습 시키기\n",
    "rf.fit(X_train, y_train)\n",
    "# 트리(100개)들의 특성별 중요도 수치화\n",
    "\n",
    "'''\n",
    "이전 단일트리 특성 중요도: [0.12345626 0.86862934 0.0079144 ]\n",
    "랜덤 포레스트 특성 중요도: [0.23167441 0.50039841 0.26792718]\n",
    "\n",
    "노드마다 랜덤하게 특성들을 뽑아서 사용 -> 다양한 특성들이 훈련에서 기준으로 쓰임\n",
    "-> 특정 특성에 과도하게 집중 방지 -> 다양한 특성이 훈련에 쓰일 기회를 받음 -> Overfitting을 줄이고 일반화 성능 높임\n",
    "'''\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a044d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOB(Out Of Bag) 샘플. -> 쓰지않은 샘플 존재 -> 남은 데이터들을 모아서 만든 샘플 (훈련에 참여 안한 샘플)\n",
    "# OOB샘플로 점수 -> 마치 검증샘플 같은 역할\n",
    "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(rf.score(X_train, y_train), rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6031fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤 포레스트의 하이퍼 파라미터 튜닝\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "\n",
    "params = {\n",
    "    # Decision Tree 의 HP\n",
    "    'max_depth': randint(3, 50),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    # Random Forest 의 HP\n",
    "    'max_features': ['sqrt', 'log2', None],  # 노드별 분할시 고려할 특성 수 (None->모든 특성 고려)\n",
    "    'n_estimators': randint(50, 300)  # 트리 개수\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(\n",
    "    rf,\n",
    "    params,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rs.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('최적 파라미터', rs.best_params_)\n",
    "print('최고 교차검증 점수', rs.best_score_)\n",
    "print('테스트 스코어', rs.score(X_test, y_test))\n",
    "# 최적 모델 인스턴스\n",
    "best_rf = rs.best_estimator_\n",
    "print('특성 중요도', best_rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1b1a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results = pd.DataFrame(rs.cv_results_)\n",
    "\n",
    "# max_depth vs mean_test_score\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(results['param_max_depth'], results['mean_test_score'], marker='o')\n",
    "plt.xlabel(\"max_depth\")\n",
    "plt.ylabel(\"Mean CV Score\")\n",
    "plt.title(\"Effect of max_depth on Model Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b9c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "pivot_table = results.pivot_table(values='mean_test_score',\n",
    "                                  index='param_max_depth',\n",
    "                                  columns='param_n_estimators')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='viridis')\n",
    "plt.title(\"Hyperparameter Search Results (CV Score)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = rs.best_estimator_\n",
    "importances = pd.Series(best_rf.feature_importances_, index=X.columns)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "importances.sort_values().plot(kind='barh')\n",
    "plt.title(\"Feature Importances (Best RandomForest)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba8b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차검증 결과 분포\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x='param_max_depth', y='split0_test_score', data=results)\n",
    "plt.title(\"CV Score Distribution by max_depth\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48470ce",
   "metadata": {},
   "source": [
    "## Extra Tree\n",
    "- 랜덤 포레스트와 매우 유사\n",
    "- 부트스트랩 샘플(복원추출)을 사용하지 않음\n",
    "- 전체 훈련세트 그대로 사용함. 특성도 무작위 선택\n",
    "- 노드 분할할 때, 최적(불순도/정보이득)을 찾는것이 아니라 무작위로 분할\n",
    "- 성능이 낮아질 수 있지만, 많은 트리를 앙상블 해서 과대적합을 방지\n",
    "- 데이터 노이즈가 많은 경우 장점 (노이즈: 패턴과 상관없는 불필요/잘못된 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46639bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# 모델에서 CPU 최대활용\n",
    "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "# 모델 교차검증 CPU 최대활용\n",
    "scores = cross_validate(et, X_train, y_train, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "et.fit(X_train, y_train)\n",
    "\n",
    "print(et.score(X_test, y_test), et.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd76e48",
   "metadata": {},
   "source": [
    "## 그래디언트 부스팅 (Gradient Boosting)\n",
    "\n",
    "- 부스팅: 약한 모델(결정트리가 얕다)을 여러 개 차례대로 학습.\n",
    "- 앞에 모델에서 틀린 부분을 뒤의 모델이 보완해주는 방식\n",
    "- 그래디언트: 오차를 줄이기 위해 경사하강법 아이디어를 적용\n",
    "### 장점\n",
    "- 비선형, 복잡한 데이터에서 예측이 뛰어남\n",
    "- 과적합 방지 가능한 여러 규제들이 있음\n",
    "- 별도 전용 라이브러리(Hist 기반: XGBoost, LightGBM, CatBoost) -> 대회에서 1등먹은 적이 많음\n",
    "### 단점\n",
    "- 학습속도 느림 (순차학습 -> 병렬화 불가능)\n",
    "- 하이퍼파라미터가 많아서 튜닝이 필요\n",
    "- 데이터 노이즈에 민감한 편\n",
    "\n",
    "### 동작\n",
    "1. 약한 첫번째 결정트리 학습 -> 기본 예측값 생성\n",
    "2. 오차/잔차(residual) 계산 (답 - 예측)\n",
    "3. 이 잔차를 예측하도록 또 다른 작은 트리를 학습 -> 예측값 업데이트\n",
    "4. 반복 -> 오차가 줄어듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10598219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(gb, X_train, y_train, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "# 학습 -> 테스트 셋 점수\n",
    "gb.fit(X_train, y_train)\n",
    "print(gb.score(X_test, y_test))\n",
    "\n",
    "# RF : 0.9973541965122431 0.8905151032797809\n",
    "# ET : 0.9974503966084433 0.8887848893166506\n",
    "# GB : 0.8881086892152563 0.8720430147331015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f2919",
   "metadata": {},
   "source": [
    "## 히스토그램 기반 그래디언트 부스팅\n",
    "\n",
    "- 입력 특성들을 256개 구간으로 나눔\n",
    "- 노드 분할 할때, 구간 경계값(256개)을 쓰기에 최적분할을 가장 빠르게 찾을 수 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfef37",
   "metadata": {},
   "source": [
    "**Hist Gradient Boosting 은 결측치를 자동으로 채워주는 기능이 있다**\n",
    "\n",
    "### 1. 직접 결측치 채우기 (Imputation)\n",
    "\n",
    "방법: 평균/중앙값 대체, KNN imputation, MICE 등\n",
    "\n",
    "##### ✅ 장점\n",
    "- 모델에 독립적: 어떤 모델이든(트리, 선형, 신경망) 학습할 수 있게 됨\n",
    "- 도메인 지식 반영 가능: 예를 들어 나이가 결측이면 \"20대 평균값\"으로 채우는 식으로 의미 있게 대체 가능\n",
    "- 노이즈 제어 가능: 결측치가 너무 많을 때 모델이 이상하게 학습하지 않도록 사전에 통제 가능\n",
    "\n",
    "##### ⚠️ 단점\n",
    "- 잘못된 방법으로 채우면 편향이 생길 수 있음 (예: 평균 대체 → 분산 축소, 패턴 왜곡)\n",
    "- 고차원 데이터에서 계산량이 크거나 불안정할 수 있음 (특히 MICE, KNN)\n",
    "---\n",
    "#### 2. HistGradientBoosting에 맡기기 (자동 결측 처리)\n",
    "\n",
    "##### ✅ 장점\n",
    "- 추가 전처리 불필요: NaN 그대로 넣으면 학습 가능 → 파이프라인 단순화\n",
    "- 모델이 결측 그 자체를 패턴으로 활용 가능 (예: “나이 없음” 자체가 중요한 특징일 때 유용)\n",
    "- 빠르고 효율적 (히스토그램 기반 트리라 속도도 빠름)\n",
    "##### ⚠️ 단점\n",
    "- 다른 모델에 적용 불가 (결측치 처리 기능이 없는 모델과 비교 불가)\n",
    "- 결측치가 너무 많으면, 모델이 “NaN 브랜치”에 데이터를 몰아넣어서 과적합 위험 있음\n",
    "- 도메인 지식을 반영하기 어려움 (모델이 무작정 NaN을 또 하나의 값으로 취급)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377e594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321723946453317 0.8801241948619236\n",
      "0.8723076923076923\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(hgb, X_train, y_train, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "# 학습 -> 테스트 셋 점수\n",
    "hgb.fit(X_train, y_train)\n",
    "print(hgb.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fb7a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9567059184812372 0.8783915747390243\n",
      "0.8746153846153846\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
    "\n",
    "scores = cross_validate(xgb, X_train, y_train, return_train_score=True)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "print(xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b73389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935828414851749 0.8801251203079884\n",
      "0.8730769230769231\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb = LGBMClassifier(random_state=42)\n",
    "\n",
    "scores = cross_validate(lgb, X_train, y_train, return_train_score=True)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
    "\n",
    "lgb.fit(X_train, y_train)\n",
    "print(lgb.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
