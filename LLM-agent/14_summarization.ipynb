{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea7777a",
   "metadata": {},
   "source": [
    "# 요약 (Summarization )\n",
    "`14_summarization.ipynb`\n",
    "\n",
    "매우 많은 양의 컨텍스트가 있을 경우, 어떻게 요약을 해야 할까?\n",
    "1. 프롬프트에 다 때려 박기\n",
    "2. Map-Reduce: 각 문서를 요약하고, 이것들을 다 합쳐서 최종 요약본을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d314897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e182ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query='reasoning',\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "docs = docs[:2]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9072fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b15642",
   "metadata": {},
   "source": [
    "## 문서 때려 박기 (Stuff Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a2e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '아래 내용을 정확하게 요약해: \\n\\n{context}')\n",
    "])\n",
    "\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "res = chain.invoke({'context': docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc866680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아래 두 논문의 핵심 내용을 각각 정확하게 요약합니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Process or Result? Manipulated Ending Tokens Can Mislead Reasoning LLMs to Ignore the Correct Reasoning Steps**\n",
      "\n",
      "**핵심 요약:**\n",
      "\n",
      "- **연구 목적:** 최근 수리 추론에 강한 LLM(대형 언어모델)들은 Chain-of-Thought(CoT) 방식으로 자기 검증 및 오류 수정 능력이 향상되었으나, 입력된 추론 과정(Reasoning Tokens)에 미세한 오류가 포함될 경우 얼마나 취약한지 분석함.\n",
      "- **주요 발견:** \n",
      "  - **Compromising Thought (CPT) 현상:** 추론 과정의 마지막 계산 결과(ending tokens)만 살짝 조작해도, LLM은 올바른 추론 과정을 무시하고 조작된(틀린) 결과를 최종 답으로 채택하는 경향이 강함.\n",
      "  - **실험 결과:** 여러 LLM(DeepSeek-R1, OpenAI-o1, o3-mini 등)에서 CPT 저항성을 세 가지 방법(불확실성 유도, 명시적 오류 지시, 출력 프리픽스 강제)으로 측정했으나, 대부분 모델이 조작된 결과를 잘못 받아들이고 자기 수정에 실패함.\n",
      "  - **기존 연구와의 차이:** 기존에는 추론 구조(구성) 변화가 내용(결과) 변화보다 모델 성능에 더 큰 영향을 준다고 했으나, 본 연구는 마지막 결과 토큰의 미세 조작이 구조 변화보다 더 치명적임을 보임.\n",
      "  - **보안 취약점:** DeepSeek-R1은 조작된 reasoning tokens가 입력되면 아예 추론을 멈추고 답을 내지 않는 현상(“thinking stopped”)이 발생함.\n",
      "- **의의:** LLM의 자기 수정 및 추론 견고성에 심각한 취약점이 있음을 밝히고, 수리/추론 중심 응용에서 보안상 주의가 필요함을 강조함.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **Hypothesis Testing Prompting Improves Deductive Reasoning in Large Language Models**\n",
      "\n",
      "**핵심 요약:**\n",
      "\n",
      "- **연구 목적:** 기존 Chain-of-Thought(CoT) 프롬프트 방식이 복잡한 논리 추론에서 한계(비합리적/허구적 추론 경로 등)를 보이므로, 이를 개선하기 위해 ‘가설 검증 프롬프트(Hypothesis Testing Prompting)’를 제안함.\n",
      "- **방법:** \n",
      "  - 결론에 대한 가설을 세우고(참/거짓 가정), 역방향 추론 및 사실 검증을 중간 단계에 포함시킴.\n",
      "  - 각 결론에 대해 “참이라고 가정하고 역추론→조건 검증, 거짓이라고 가정하고 역추론→조건 검증” 과정을 거쳐 유일한 정답에 도달하도록 유도함.\n",
      "- **실험:** \n",
      "  - 두 논리 추론 데이터셋(RuleTaker, ProofWriter)에서 실험.\n",
      "  - 기존 표준 프롬프트, CoT 프롬프트와 비교 시, Hypothesis Testing Prompting이 특히 복잡한(다단계) 추론에서 정확도와 추론 과정의 합리성 모두에서 우수함을 보임.\n",
      "  - 특히 “Unknown(불확실)” 정답 분류에서 CoT 대비 월등한 성능을 보임.\n",
      "- **의의:** LLM이 더 논리적이고 표준화된 추론 과정을 생성하도록 유도할 수 있으며, 복잡한 논리 추론 문제에서 성능과 해석 가능성을 동시에 높임.\n",
      "\n",
      "---\n",
      "\n",
      "**요약 비교:**  \n",
      "- 첫 논문은 LLM이 입력 추론 과정의 마지막 결과만 살짝 조작해도 쉽게 속아 넘어가며, 자기 수정 능력이 실제로는 취약함을 보임(특히 수리 문제에서).\n",
      "- 두 번째 논문은 LLM의 논리 추론 능력을 높이기 위해, 결론 가정→역추론→사실 검증의 구조적 프롬프트(Hypothesis Testing Prompting)를 도입해, 기존 CoT 방식보다 더 정확하고 합리적인 추론을 유도함을 보임.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898392",
   "metadata": {},
   "source": [
    "## Map - Reduce\n",
    "- 각각 나눠서 요약하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e641703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '아래 내용을 정확하게 요약해: \\n\\n{context}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79858772",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_template = \"\"\"\n",
    "아래에 요약된 문서들이야.\n",
    "\n",
    "{docs}\n",
    "---\n",
    "이것들을 가지고 정제해서 최종 통합본을 잘 만들어줘.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([\n",
    "    ('human', reduce_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f8acc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query='reasoning')\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56c1993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86524b65",
   "metadata": {},
   "source": [
    "### Langgrpah로 문서별 요약 작업 조율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f0132a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1003, which is longer than the specified 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서를 더 작은 문서로 쪼개기\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,  # tiktoken 인코더라 토큰 기준 1000개\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
