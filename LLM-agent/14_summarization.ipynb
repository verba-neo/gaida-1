{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ea7777a",
   "metadata": {},
   "source": [
    "# 요약 (Summarization )\n",
    "`14_summarization.ipynb`\n",
    "\n",
    "매우 많은 양의 컨텍스트가 있을 경우, 어떻게 요약을 해야 할까?\n",
    "1. 프롬프트에 다 때려 박기\n",
    "2. Map-Reduce: 각 문서를 요약하고, 이것들을 다 합쳐서 최종 요약본을 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d314897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d2fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e182ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader, ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "    query='reasoning',\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "docs = docs[:2]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b15642",
   "metadata": {},
   "source": [
    "## 문서 때려 박기 (Stuff Docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '아래 내용을 정확하게 요약해: \\n\\n{context}')\n",
    "])\n",
    "\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "res = chain.invoke({'context': docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc866680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39898392",
   "metadata": {},
   "source": [
    "## Map - Reduce\n",
    "- 각각 나눠서 요약하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8acc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query='reasoning')\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86524b65",
   "metadata": {},
   "source": [
    "### Langgrpah로 문서별 요약 작업 조율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0132a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 더 작은 문서로 쪼개기\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000,  # tiktoken 인코더라 토큰 기준 1000개\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "len(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dd1b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing_extensions import Annotated, List, Literal, TypedDict\n",
    "\n",
    "from langchain.chains.combine_documents.reduce import acollapse_docs, split_list_of_docs\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.types import Send\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "\n",
    "# 전체적으로 사용할 State (Reduce)\n",
    "class OverallState(TypedDict):\n",
    "    contents: List[str]                         # 입력 문서 조각의 내용들\n",
    "    summaries: Annotated[list, operator.add]    # 각 contents의 요약본 (노드들이 여러개의 요약을 반환하면, 자동으로 리스트에 합쳐짐)\n",
    "    collapsed_summaries: List[Document]         # summaries를 Document로 포장한 것들\n",
    "    final_summary: str                          # 최종 요약본\n",
    "\n",
    "\n",
    "# 개별 문서를 처리할 State (Map)\n",
    "class SummaryState(TypedDict):\n",
    "    content: str    # 각 문서를 요약할 때 사용할 문서의 내용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "TOKEN_MAX = 2000\n",
    "\n",
    "map_prmopt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Write a concise summary of the following:\\\\n\\\\n{context}\")\n",
    "])\n",
    "\n",
    "reduce_template = \"\"\"\n",
    "The following is a set of summaries:\n",
    "{docs}\n",
    "Take these and distill it into a final, consolidated summary\n",
    "of the main themes. \n",
    "Answer in Korean.\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = ChatPromptTemplate([\n",
    "    ('human', reduce_template)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc095ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node, Router 아닌 실제 사용할 함수들\n",
    "\n",
    "async def _reduce(input: dict) -> str:\n",
    "    prompt = reduce_prompt.invoke(input)\n",
    "    res = await llm.ainvoke(prompt)\n",
    "    return res.content\n",
    "\n",
    "# documents 인자 내부의 모든 내용의 토큰 총 합\n",
    "def sum_docs_tokens(documents: List[Document]) -> int:\n",
    "    return sum(llm.get_num_tokens(doc.page_content) for doc in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge (router) -> 각 원본조각을 요약할수 있게 generate_summary 로 보냄 (문서 조각 개수만큼)\n",
    "def map_summaries(state: OverallState):\n",
    "    result = []\n",
    "    for content in state['contents']:\n",
    "        result.append(Send('generate_summary', {'content': content}))\n",
    "    return result  # List Comprehension 으로 교체 가능\n",
    "\n",
    "\n",
    "# Edge (router) -> 재귀적으로 계속 collapse_suammries 를 할지, 끝낼지 결정하는 라우터\n",
    "def should_collapse(\n",
    "    state: OverallState,\n",
    ") -> Literal[\"collapse_summaries\", \"generate_final_summary\"]:\n",
    "    num_tokens = sum_docs_tokens(state[\"collapsed_summaries\"])\n",
    "    if num_tokens > TOKEN_MAX:\n",
    "        return \"collapse_summaries\"\n",
    "    else:\n",
    "        return \"generate_final_summary\"\n",
    "\n",
    "\n",
    "# Node: 주어진 내용을 요약함. (비동기적 실행)\n",
    "async def generate_summary(state: SummaryState):\n",
    "    prompt = map_prmopt.invoke({'context': state['content']})\n",
    "    res = await llm.ainvoke(prompt)\n",
    "    return {'summaries': [res.content]}\n",
    "\n",
    "\n",
    "# Node: 위에서 생성한 요약들을 Document() 객체로 만들어서 'collapsed_summaries' 키에 넣어줌\n",
    "def collect_summaries(state: OverallState):\n",
    "    return {\n",
    "        'collapsed_summaries': [Document(summary) for summary in state['summaries']]\n",
    "    }\n",
    "\n",
    "\n",
    "# Node: 1차 요약이 완료. -> 요약본이 토큰수가 너무 많을 수 있다 -> 필요에 따라 더 작은 요약으로 축소(collapse)\n",
    "async def collapse_summaries(state: OverallState):\n",
    "    docs_lists = split_list_of_docs(\n",
    "        state['collapsed_summaries'],\n",
    "        sum_docs_tokens,\n",
    "        TOKEN_MAX\n",
    "    )\n",
    "    results = []\n",
    "    for doc_list in docs_lists:\n",
    "        results.append(await acollapse_docs(doc_list, _reduce))\n",
    "\n",
    "    return {'collapsed_summaries': results}\n",
    "\n",
    "\n",
    "# Node: 최종 정리 노드\n",
    "async def generate_final_summary(state: OverallState):\n",
    "    response = await _reduce(state[\"collapsed_summaries\"])\n",
    "    return {\"final_summary\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c348959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the graph\n",
    "# Nodes:\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_summary\", generate_summary)  # same as before\n",
    "graph.add_node(\"collect_summaries\", collect_summaries)\n",
    "graph.add_node(\"collapse_summaries\", collapse_summaries)\n",
    "graph.add_node(\"generate_final_summary\", generate_final_summary)\n",
    "\n",
    "# Edges:\n",
    "graph.add_conditional_edges(START, map_summaries, [\"generate_summary\"])\n",
    "graph.add_edge(\"generate_summary\", \"collect_summaries\")\n",
    "graph.add_conditional_edges(\"collect_summaries\", should_collapse)\n",
    "graph.add_conditional_edges(\"collapse_summaries\", should_collapse)\n",
    "graph.add_edge(\"generate_final_summary\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async for step in app.astream(\n",
    "    {'contents': [doc.page_content for doc in split_docs]},\n",
    "    {'recursion_limit': 10}\n",
    "):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e4dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1deb0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
